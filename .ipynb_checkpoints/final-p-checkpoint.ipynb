{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from nltk.corpus import wordnet as wn\n",
    "import operator\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn import preprocessing \n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating documents list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# documents=[]\n",
    "# errs=0\n",
    "# docs_no=300\n",
    "# lid=listdir('dataset/blogs/')\n",
    "# for l in lid:\n",
    "#     if docs_no==0:\n",
    "#         break\n",
    "#     f = open('dataset/blogs/'+l, \"r\")\n",
    "#     try:\n",
    "#         text=f.read()\n",
    "#     except:\n",
    "#         errs+=1\n",
    "#         continue\n",
    "#     text=text.split('</post>')\n",
    "#     for t in text:\n",
    "#         documents.append(t) # each post is a document\n",
    "#     docs_no-=1\n",
    "\n",
    "# for d in range(len(documents)):\n",
    "#     documents[d]= documents[d].split() # exploit thhe words os a doc\n",
    "\n",
    "# for i in range(len(documents)):\n",
    "#     for j in reversed(range(len(documents[i]))):\n",
    "#         if documents[i][j].endswith('>') or documents[i][j].startswith('<'):\n",
    "#             documents[i].remove(documents[i][j]) # removing xml tags\n",
    "        \n",
    "\n",
    "#------------------------------------------------\n",
    "f = open('/kaggle/input/wikipedia-sentences/wikisent2.txt', \"r\")\n",
    "try:\n",
    "    documents=f.readlines()\n",
    "except:\n",
    "    print('failed!')\n",
    "\n",
    "for d in range(len(documents)):\n",
    "    documents[d]= documents[d].split()\n",
    "for d in range(len(documents)):\n",
    "    documents[d].remove(documents[d][0])\n",
    "\n",
    "documents=documents[:25000] # just taking 50000 docs\n",
    "\n",
    "#---------------------------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/nlp-topic-modelling/Reviews.csv\")\n",
    "saved_column = df.Text\n",
    "\n",
    "for a in saved_column:\n",
    "    c=a.split()\n",
    "    documents.append(c)\n",
    "    \n",
    "documents=documents[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating transition matrix for the documents list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition(documents):\n",
    "    words=set()\n",
    "    end_words=set()\n",
    "    for d in range(len(documents)):\n",
    "        for w in range(len(documents[d])):\n",
    "            if len(documents[d][w])>0:\n",
    "                if documents[d][w][-1] in ['.','?','!']:\n",
    "                    end_words.add(documents[d][w])\n",
    "            words.add(documents[d][w])\n",
    "\n",
    "    #matrix initialization\n",
    "\n",
    "    transition_matrix={}\n",
    "    for i in words:\n",
    "        transition_matrix[i]=[]  #transition_matrix[i]=[[freq,'word'],[freq,'word']]\n",
    "\n",
    "    # scan the documents and fill transition matrix\n",
    "\n",
    "    for d in range(len(documents)):\n",
    "        for w in range(len(documents[d])):\n",
    "\n",
    "            if w==len(documents[d])-1:\n",
    "                break #if its last word come out\n",
    "\n",
    "            found=False\n",
    "            ind=0\n",
    "\n",
    "            for i in transition_matrix[documents[d][w]]:\n",
    "                if documents[d][w+1] in i:\n",
    "                    ind=transition_matrix[documents[d][w]].index(i)\n",
    "                    found=True # check if its alredy appended & find the index\n",
    "\n",
    "            if found==False:\n",
    "                transition_matrix[documents[d][w]].append([1,documents[d][w+1]])\n",
    "            if found==True:\n",
    "                transition_matrix[documents[d][w]][ind][0]+=1\n",
    "\n",
    "    # incidents---> probability\n",
    "    for a in transition_matrix:\n",
    "        sum=0\n",
    "        count=len(transition_matrix[a])\n",
    "        for b in transition_matrix[a]:\n",
    "            sum+=b[0]\n",
    "        for b in transition_matrix[a]:\n",
    "            b[0]/=sum\n",
    "        transition_matrix[a].sort(reverse=True)\n",
    "    return (transition_matrix,words,end_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving matrix, docs, words and endwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=make_transition(documents)\n",
    "transition_matrix=t[0]\n",
    "words=t[1]\n",
    "end_words=t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"matrix.pickle\",\"wb\")\n",
    "pickle.dump(transition_matrix, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"docs.pickle\",\"wb\")\n",
    "pickle.dump(documents, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"words.pickle\",\"wb\")\n",
    "pickle.dump(words, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"ends.pickle\",\"wb\")\n",
    "pickle.dump(end_words, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading matrix, docs, word and endwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"matrix.pickle\",\"rb\")\n",
    "transition_matrix = pickle.load(pickle_in)\n",
    "pickle_in = open(\"docs.pickle\",\"rb\")\n",
    "documents = pickle.load(pickle_in)\n",
    "pickle_in = open(\"words.pickle\",\"rb\")\n",
    "words = pickle.load(pickle_in)\n",
    "pickle_in = open(\"ends.pickle\",\"rb\")\n",
    "end_words = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating next word for text generatin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(word,transition_matrix):\n",
    "    my_word=word\n",
    "    ws=[]\n",
    "    ps=[]\n",
    "    if my_word not in transition_matrix:\n",
    "        return '-'\n",
    "    if len(transition_matrix[my_word])==0:\n",
    "        return '-'\n",
    "    for i in transition_matrix[my_word]:\n",
    "        ps.append(i[0])\n",
    "        ws.append(i[1])\n",
    "    r=np.random.choice(ws, 1, p=ps)\n",
    "    #transition_matrix[my_word]\n",
    "    return r[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeling that does have tried them and very artful, and Kit-kat, they come to still a pound at Saint-Pol-sur-Mer, France, one was also yummy. "
     ]
    }
   ],
   "source": [
    "#generating word until end-word\n",
    "jj=[]\n",
    "ss='feeling'\n",
    "jj.append(ss)\n",
    "while ss not in end_words:\n",
    "    ss=next_word(ss,transition_matrix)\n",
    "    jj.append(ss)\n",
    "for f in jj:\n",
    "    print(f,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make k guesses for next word based on given transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_words(transition_matrix,word,k):\n",
    "    ans=[]\n",
    "    count=0\n",
    "    if word not in transition_matrix:\n",
    "        print('no item found!')\n",
    "        return []\n",
    "    if len(transition_matrix[word])>=k:\n",
    "        for i in range(k):\n",
    "            count+=transition_matrix[word][i][0]\n",
    "            ans.append(transition_matrix[word][i])\n",
    "    else:\n",
    "        t=len(transition_matrix[word])\n",
    "        for i in range(t):\n",
    "            count+=transition_matrix[word][i][0]\n",
    "            ans.append(transition_matrix[word][i])\n",
    "    \n",
    "    print('total probability for first '+str(k)+' words is : '+str(count*100))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total probability for first 10 words is : 100.00000000000003\n",
      "[[0.3333333333333333, 'the'], [0.2222222222222222, 'of'], [0.1111111111111111, 'your'], [0.1111111111111111, 'on'], [0.1111111111111111, 'in'], [0.1111111111111111, 'exercised']]\n"
     ]
    }
   ],
   "source": [
    "print(k_words(transition_matrix,'influence',10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectral clustering the documents based on tf-idf cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making similarity matrix for documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_similarity(documents):\n",
    "#     corpus = documents.copy()\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     X = vectorizer.fit_transform(corpus) # each documents is a vector of words --> tf-idf rank for each word\n",
    "#     #print(vectorizer.get_feature_names())\n",
    "#     X=X.todense()\n",
    "#     #print(X) # if-idf doc vectors\n",
    "#     print(cosine_similarity(X, X).shape)\n",
    "#     return cosine_similarity(X, X) # cosine similarity between doc vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clustering the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each doc = (list of words)--->(a long string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd=documents.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dd)):\n",
    "#     a=''\n",
    "#     for j in dd[i]:\n",
    "#         a+=(' '+j)\n",
    "#     dd[i]=a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group 50000 docs into 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c=make_similarity(dd[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_no=10\n",
    "# labels=SpectralClustering(n_clusters=clusters_no,affinity='precomputed').fit_predict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels=list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters=[[]for i in range(clusters_no)]\n",
    "# for i in range(len(labels)):\n",
    "#     clusters[labels[i]].append(documents[i]) #specifying each clusters documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means clustering the documents based on tf-idf cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=documents.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(dd)):\n",
    "    a=''\n",
    "    for j in dd[i]:\n",
    "        if j not in stop_words:\n",
    "            a+=(' '+j)\n",
    "    dd[i]=a\n",
    "corpus = dd\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus) # each documents is a vector of words --> tf-idf rank for each word\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "X_Norm = preprocessing.normalize(X)\n",
    "\n",
    "clusters_no=10\n",
    "kmeans=KMeans(n_clusters=clusters_no).fit(X_Norm)\n",
    "labels=list(kmeans.labels_)\n",
    "\n",
    "clusters=[[]for i in range(clusters_no)]\n",
    "for i in range(len(labels)):\n",
    "    clusters[labels[i]].append(documents[i]) #specifying each clusters documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labeling using wrdnet (most frequent hypernyms of words in the document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels=[{} for i in range(clusters_no) ]\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    y=X[i].todense().tolist()\n",
    "    ind=y[0].index(max(y[0]))\n",
    "    mw=o[ind]\n",
    "    \n",
    "    dictof=cluster_labels[labels[i]]\n",
    "    \n",
    "    if len(wn.synsets(mw))==0:\n",
    "        continue\n",
    "    a=wn.synsets(mw)[0]\n",
    "    for b in a.hypernyms():\n",
    "        for c in b.lemma_names():\n",
    "            if c in dictof:\n",
    "                dictof[c]+=1\n",
    "            else:\n",
    "                dictof[c]=1\n",
    "for a in range(len(cluster_labels)):\n",
    "    sorted_dictof = sorted(cluster_labels[a].items(), key=operator.itemgetter(1),reverse=True)\n",
    "    cluster_labels[a]=sorted_dictof\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save clusters and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"clusters.pickle\",\"wb\")\n",
    "pickle.dump(clusters, pickle_out)\n",
    "\n",
    "pickle_out = open(\"clusterlabels.pickle\",\"wb\")\n",
    "pickle.dump(cluster_labels, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load clusters and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"clusters.pickle\",\"rb\")\n",
    "clusters = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"clusterlabels.pickle\",\"rb\")\n",
    "cluster_labels = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# cluster_labels=[]\n",
    "# for each_cluster in clusters:\n",
    "#     dictof={}\n",
    "#     for each_doc in each_cluster:\n",
    "#         #ea = word_tokenize(each_doc)\n",
    "#         _doc = [w for w in each_doc if not w in stop_words]\n",
    "#         for each_word in _doc:\n",
    "#             w=each_word\n",
    "#             if len(w)==1:\n",
    "#                 continue\n",
    "#             if len(wn.synsets(w))==0:\n",
    "#                 continue\n",
    "#             a=wn.synsets(w)[0]\n",
    "#             for b in a.hypernyms():\n",
    "#                 for c in b.lemma_names():\n",
    "#                     if c in dictof:\n",
    "#                         dictof[c]+=1\n",
    "#                     else:\n",
    "#                         dictof[c]=1\n",
    "#     sorted_dictof = sorted(dictof.items(), key=operator.itemgetter(1),reverse=True)\n",
    "#     cluster_labels.append(sorted_dictof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making transition matrix of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_transitions=[]\n",
    "for each_cluster in clusters:\n",
    "    docs=[]\n",
    "    mat=[]\n",
    "#     for each_doc in each_cluster:\n",
    "#         each_doc=each_doc.split(' ')\n",
    "#         docs.append(each_doc)\n",
    "#     mat=make_transition(docs)\n",
    "    mat=make_transition(each_cluster)\n",
    "    clusters_transitions.append(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save cluster transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"clustertransitions.pickle\",\"wb\")\n",
    "pickle.dump(clusters_transitions, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load cluster transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"clustertransitions.pickle\",\"rb\")\n",
    "clusters_transitions = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the total documents :\n",
      "next word: \n",
      "total probability for first 10 words is : 55.834945196647325\n",
      "[[0.16634429400386846, 'to'], [0.07672469374597034, 'back'], [0.06447453255963895, 'for'], [0.06318504190844616, 'through'], [0.06125080593165699, 'wrong'], [0.04577691811734365, 'with'], [0.025789813023855575, 'out'], [0.019987105093488073, 'on'], [0.018052869116698903, 'into'], [0.016763378465506126, 'crazy']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 0 :\n",
      "labels :\n",
      "association, orthopterous_insect, orthopteron, orthopteran, vessel, field_game, football, football_game, motor_vehicle, automotive_vehicle, \n",
      "next word: \n",
      "no item found!\n",
      "[]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 1 :\n",
      "labels :\n",
      "canine, canid, domestic_animal, domesticated_animal, drink, cake, potato, white_potato, Irish_potato, murphy, \n",
      "next word: \n",
      "total probability for first 10 words is : 54.59704880817253\n",
      "[[0.1623155505107832, 'to'], [0.08059023836549375, 'back'], [0.0681044267877412, 'wrong'], [0.06242905788876277, 'through'], [0.05448354143019296, 'for'], [0.03859250851305335, 'with'], [0.02383654937570942, 'crazy'], [0.022701475595913734, 'out'], [0.0170261066969353, 'on'], [0.015891032917139614, 'into']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 2 :\n",
      "labels :\n",
      "drink, beverage, drinkable, potable, container, crockery, dishware, case, face, coffee, \n",
      "next word: \n",
      "total probability for first 10 words is : 66.4864864864865\n",
      "[[0.1837837837837838, 'to'], [0.11351351351351352, 'back'], [0.0972972972972973, 'for'], [0.07027027027027027, 'through'], [0.05945945945945946, 'with'], [0.05405405405405406, 'wrong'], [0.02702702702702703, 'out'], [0.02702702702702703, 'down'], [0.016216216216216217, 'without'], [0.016216216216216217, 'well']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 3 :\n",
      "labels :\n",
      "large_integer, contestant, adult, grownup, female, female_person, oppose, contact_sport, barrier, digit, \n",
      "next word: \n",
      "no item found!\n",
      "[]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 4 :\n",
      "labels :\n",
      "court_game, European, Gregorian_calendar_month, dynasty, royalty, royal_family, royal_line, royal_house, honor, honour, \n",
      "next word: \n",
      "no item found!\n",
      "[]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 5 :\n",
      "labels :\n",
      "beverage, drink, drinkable, potable, herb, herbaceous_plant, shrub, bush, chromatic_color, chromatic_colour, \n",
      "next word: \n",
      "total probability for first 10 words is : 72.36842105263158\n",
      "[[0.2236842105263158, 'to'], [0.09210526315789473, 'for'], [0.09210526315789473, 'back'], [0.06578947368421052, 'wrong'], [0.06578947368421052, 'with'], [0.06578947368421052, 'through'], [0.039473684210526314, 'into'], [0.02631578947368421, 'this'], [0.02631578947368421, 'out'], [0.02631578947368421, 'by']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 6 :\n",
      "labels :\n",
      "Gregorian_calendar_month, cavalry, athletics, army_unit, sport, large_integer, person, individual, someone, somebody, \n",
      "next word: \n",
      "total probability for first 10 words is : 96.15384615384613\n",
      "[[0.34615384615384615, 'to'], [0.15384615384615385, 'directly'], [0.11538461538461539, 'on'], [0.07692307692307693, 'wrong'], [0.07692307692307693, 'one'], [0.038461538461538464, 'out'], [0.038461538461538464, 'on,'], [0.038461538461538464, 'of'], [0.038461538461538464, 'into'], [0.038461538461538464, 'for']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 7 :\n",
      "labels :\n",
      "vote, administrative_district, administrative_division, territorial_division, dynasty, royalty, royal_family, royal_line, royal_house, legislature, \n",
      "next word: \n",
      "no item found!\n",
      "[]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 8 :\n",
      "labels :\n",
      "element, chemical_element, halogen, nutriment, nourishment, nutrition, sustenance, aliment, alimentation, victuals, \n",
      "next word: \n",
      "total probability for first 10 words is : 54.83028720626631\n",
      "[[0.14360313315926893, 'to'], [0.06788511749347259, 'for'], [0.06527415143603134, 'through'], [0.05483028720626632, 'with'], [0.05221932114882506, 'back'], [0.04699738903394256, 'wrong'], [0.03655352480417755, 'ahead'], [0.031331592689295036, 'out'], [0.02610966057441253, 'on'], [0.02349869451697128, 'into']]\n",
      "------------------------------------------------------------------------\n",
      "cluster number 9 :\n",
      "labels :\n",
      "set_forth, expound, exposit, struggle, diversion, recreation, Amerindian, Native_American, \n",
      "next word: \n",
      "no item found!\n",
      "[]\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "your_word='go'\n",
    "print('for the total documents :')\n",
    "print('next word: ')\n",
    "print(k_words(transition_matrix,your_word,10))\n",
    "print('------------------------------------------------------------------------')\n",
    "for clno in range(clusters_no):\n",
    "    print('cluster number '+str(clno)+' :')\n",
    "    print('labels :')\n",
    "    ii=cluster_labels[clno][:10]\n",
    "    for i in ii:\n",
    "        print(i[0]+', ',end='')\n",
    "    print()\n",
    "    print('next word: ')\n",
    "    print(k_words(clusters_transitions[clno][0],your_word,10))\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of clusters : 10\n",
      "# of predicted words : 3\n",
      "total prob. of first 3 word is : 0.8617627830117676\n",
      "size of dictionary is: 129771\n",
      "---------------------------------\n",
      "# of documents in cluster 0 : 2907\n",
      "prob. of first 3 word in cluster number 0 is : 0.8216708405277435\n",
      "size of dictionary is: 4348\n",
      "labels :\n",
      "association, orthopterous_insect, orthopteron, orthopteran, vessel, field_game, football, football_game, motor_vehicle, automotive_vehicle, \n",
      "---------------------------------\n",
      "# of documents in cluster 1 : 16691\n",
      "prob. of first 3 word in cluster number 1 is : 0.8719048028622921\n",
      "size of dictionary is: 60494\n",
      "labels :\n",
      "canine, canid, domestic_animal, domesticated_animal, drink, cake, potato, white_potato, Irish_potato, murphy, \n",
      "---------------------------------\n",
      "# of documents in cluster 2 : 2784\n",
      "prob. of first 3 word in cluster number 2 is : 0.8875081474322745\n",
      "size of dictionary is: 16205\n",
      "labels :\n",
      "drink, beverage, drinkable, potable, container, crockery, dishware, case, face, coffee, \n",
      "---------------------------------\n",
      "# of documents in cluster 3 : 403\n",
      "prob. of first 3 word in cluster number 3 is : 0.95648355520438\n",
      "size of dictionary is: 276\n",
      "labels :\n",
      "large_integer, contestant, adult, grownup, female, female_person, oppose, contact_sport, barrier, digit, \n",
      "---------------------------------\n",
      "# of documents in cluster 4 : 1601\n",
      "prob. of first 3 word in cluster number 4 is : 0.9387121495976288\n",
      "size of dictionary is: 1663\n",
      "labels :\n",
      "court_game, European, Gregorian_calendar_month, dynasty, royalty, royal_family, royal_line, royal_house, honor, honour, \n",
      "---------------------------------\n",
      "# of documents in cluster 5 : 1163\n",
      "prob. of first 3 word in cluster number 5 is : 0.8971616680643815\n",
      "size of dictionary is: 11985\n",
      "labels :\n",
      "beverage, drink, drinkable, potable, herb, herbaceous_plant, shrub, bush, chromatic_color, chromatic_colour, \n",
      "---------------------------------\n",
      "# of documents in cluster 6 : 19708\n",
      "prob. of first 3 word in cluster number 6 is : 0.8205698541211167\n",
      "size of dictionary is: 47806\n",
      "labels :\n",
      "Gregorian_calendar_month, cavalry, athletics, army_unit, sport, large_integer, person, individual, someone, somebody, \n",
      "---------------------------------\n",
      "# of documents in cluster 7 : 809\n",
      "prob. of first 3 word in cluster number 7 is : 0.858524084461603\n",
      "size of dictionary is: 1214\n",
      "labels :\n",
      "vote, administrative_district, administrative_division, territorial_division, dynasty, royalty, royal_family, royal_line, royal_house, legislature, \n",
      "---------------------------------\n",
      "# of documents in cluster 8 : 3651\n",
      "prob. of first 3 word in cluster number 8 is : 0.9041579226073497\n",
      "size of dictionary is: 47735\n",
      "labels :\n",
      "element, chemical_element, halogen, nutriment, nourishment, nutrition, sustenance, aliment, alimentation, victuals, \n",
      "---------------------------------\n",
      "# of documents in cluster 9 : 283\n",
      "prob. of first 3 word in cluster number 9 is : 0.8368978984563883\n",
      "size of dictionary is: 19\n",
      "labels :\n",
      "set_forth, expound, exposit, struggle, diversion, recreation, Amerindian, Native_American, \n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_words=len(transition_matrix)\n",
    "total=0\n",
    "k=3 # probability of first k word\n",
    "print('# of clusters : '+str(clusters_no))\n",
    "print('# of predicted words : '+str(k))\n",
    "cl_sum=[0 for i in range(clusters_no)]\n",
    "for w in transition_matrix:\n",
    "    \n",
    "    \n",
    "    if len(transition_matrix[w])>=k:\n",
    "        for i in range(k):\n",
    "            total+=transition_matrix[w][i][0]\n",
    "    else:\n",
    "        t=len(transition_matrix[w])\n",
    "        for i in range(t):\n",
    "            total+=transition_matrix[w][i][0]\n",
    "    \n",
    "\n",
    "    for i in range(clusters_no):\n",
    "        if w in clusters_transitions[i][0]:\n",
    "            if len(clusters_transitions[i][0][w])>=k:\n",
    "                for j in range(k):\n",
    "                    cl_sum[i]+=clusters_transitions[i][0][w][j][0]\n",
    "            else:\n",
    "                t=len(clusters_transitions[i][0][w])\n",
    "                for j in range(t):\n",
    "                    cl_sum[i]+=clusters_transitions[i][0][w][j][0]\n",
    "\n",
    "total=total/total_words\n",
    "print('total prob. of first '+ str(k)+' word is : '+ str(total))\n",
    "print(\"size of dictionary is: \"+ str(total_words))\n",
    "print('---------------------------------')\n",
    "for i in range(len(cl_sum)):\n",
    "    print(\"# of documents in cluster \"+str(i)+' : '+str(len(clusters[i])))\n",
    "#     cl_sum[i]=cl_sum[i]/total_words\n",
    "    cl_sum[i]=cl_sum[i]/len(clusters_transitions[i][0])\n",
    "    print('prob. of first '+ str(k)+' word in cluster number '+str(i)+' is : '+str(cl_sum[i]))\n",
    "    print(\"size of dictionary is: \"+ str(len(clusters_transitions[i][0])))\n",
    "    print('labels :')\n",
    "    ii=cluster_labels[i][:10]\n",
    "    for j in ii:\n",
    "        print(j[0]+', ',end='')\n",
    "    print('')\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### guessing the inputs cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0278e1e0537d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             raise StdinNotImplementedError(\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         return self._input_request(str(prompt),\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "line=input().split()\n",
    "w=line[-1]\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "listt=[]\n",
    "a=''\n",
    "for j in line:\n",
    "    if j not in stop_words:\n",
    "        a+=(j+' ')\n",
    "listt.append(a)\n",
    "X=vectorizer.transform(listt)\n",
    "X_Norm = preprocessing.normalize(X)\n",
    "cc=kmeans.predict(X_Norm)\n",
    "print('cluster number ',end='')\n",
    "print(cc)\n",
    "\n",
    "print('labels :')\n",
    "ii=cluster_labels[cc[0]][:20]\n",
    "for i in ii:\n",
    "    print(i[0]+', ',end='')\n",
    "print()\n",
    "print('next word: ')\n",
    "print(k_words(clusters_transitions[cc[0]][0],w,10))\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grouping based on sentiment alnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=[]\n",
    "sentiment=[]\n",
    "negs=[]\n",
    "poss=[]\n",
    "with open(\"/kaggle/input/sentimental-analysis-nlp/pos_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        poss.append(i)\n",
    "        sentence.append(i) \n",
    "        sentiment.append('positive')\n",
    "\n",
    "with open(\"/kaggle/input/sentimental-analysis-nlp/neg_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        negs.append(i)\n",
    "        sentence.append(i)\n",
    "        sentiment.append('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making transitoin matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sentence.copy()\n",
    "for d in range(len(data)):\n",
    "    data[d]= data[d].split()\n",
    "for d in range(len(negs)):\n",
    "    negs[d]= negs[d].split()\n",
    "for d in range(len(poss)):\n",
    "    poss[d]= poss[d].split()\n",
    "    \n",
    "data_transition=make_transition(data)\n",
    "negs_transition=make_transition(negs)\n",
    "poss_transition=make_transition(poss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify the poss and negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word',lowercase = False,)\n",
    "features = vectorizer.fit_transform(sentence)\n",
    "features_nd = features.toarray() \n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd, sentiment,train_size=0.90, random_state=1234)\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835820895522388\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### telling next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole datas:\n",
      "total probability for first 5 words is : 23.12703583061889\n",
      "[[0.07166123778501629, 'was'], [0.048859934853420196, 'love'], [0.04234527687296417, 'have'], [0.035830618892508145, 'cant'], [0.03257328990228013, 'think']]\n",
      "------------------------------------------------\n",
      "negative datas:\n",
      "total probability for first 5 words is : 24.782608695652172\n",
      "[[0.08260869565217391, 'was'], [0.04782608695652174, 'cant'], [0.043478260869565216, 'have'], [0.043478260869565216, 'dont'], [0.030434782608695653, 'miss']]\n",
      "------------------------------------------------\n",
      "positive datas:\n",
      "total probability for first 5 words is : 37.66233766233767\n",
      "[[0.19480519480519481, 'love'], [0.05194805194805195, 'think'], [0.05194805194805195, 'like'], [0.03896103896103896, 'was'], [0.03896103896103896, 'have']]\n"
     ]
    }
   ],
   "source": [
    "word='i'\n",
    "print('whole datas:')\n",
    "print(k_words(data_transition[0],word,5))\n",
    "print('------------------------------------------------')\n",
    "print('negative datas:')\n",
    "print(k_words(negs_transition[0],word,5))\n",
    "print('------------------------------------------------')\n",
    "print('positive datas:')\n",
    "print(k_words(poss_transition[0],word,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tell from the input sntence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-98373289dbfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             raise StdinNotImplementedError(\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         return self._input_request(str(prompt),\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "my_input=input()\n",
    "word=my_input\n",
    "word=word.split()[-1]\n",
    "\n",
    "a=[my_input]\n",
    "features = vectorizer.transform(a)\n",
    "features_nd = features.toarray() \n",
    "pred=log_model.predict(features_nd)\n",
    "print('sentiment is : '+pred[0])\n",
    "\n",
    "print('the next word would be: ')\n",
    "\n",
    "print('in whole datas:')\n",
    "print(k_words(data_transition[0],word,10))\n",
    "print('------------------------------------------------')\n",
    "print('in negative datas:')\n",
    "print(k_words(negs_transition[0],word,10))\n",
    "print('------------------------------------------------')\n",
    "print('in positive datas:')\n",
    "print(k_words(poss_transition[0],word,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### camparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prob. of first 3 word is : 0.9026035119792102\n",
      "size of dictionary is: 8627\n",
      "---------------------------------\n",
      "total pos docs : 617\n",
      "total prob. of first 3 word in positive group is : 0.9305505026169263\n",
      "size of dictionary is: 3579\n",
      "---------------------------------\n",
      "total neg dics: 1387\n",
      "total prob. of first 3 word in negative group is : 0.8991054127127222\n",
      "size of dictionary is: 6257\n"
     ]
    }
   ],
   "source": [
    "total_words=len(data_transition[0])\n",
    "total=0\n",
    "negatives=0\n",
    "positives=0\n",
    "k=3 # probability of first k word\n",
    "for w in data_transition[0]:\n",
    "    if len(data_transition[0][w])>=k:\n",
    "        for i in range(k):\n",
    "            total+=data_transition[0][w][i][0]\n",
    "    else:\n",
    "        t=len(data_transition[0][w])\n",
    "        for i in range(t):\n",
    "            total+=data_transition[0][w][i][0]\n",
    "    \n",
    "    \n",
    "for w in negs_transition[0]:\n",
    "    if len(negs_transition[0][w])>=k:\n",
    "        for i in range(k):\n",
    "            negatives+=negs_transition[0][w][i][0]\n",
    "    else:\n",
    "        t=len(negs_transition[0][w])\n",
    "        for i in range(t):\n",
    "            negatives+=negs_transition[0][w][i][0]\n",
    "            \n",
    "for w in poss_transition[0]:\n",
    "    if len(poss_transition[0][w])>=k:\n",
    "        for i in range(k):\n",
    "            positives+=poss_transition[0][w][i][0]\n",
    "    else:\n",
    "        t=len(poss_transition[0][w])\n",
    "        for i in range(t):\n",
    "            positives+=poss_transition[0][w][i][0]\n",
    "\n",
    "total=total/total_words\n",
    "print('total prob. of first '+ str(k)+' word is : '+ str(total))\n",
    "print(\"size of dictionary is: \"+ str(total_words))\n",
    "print('---------------------------------')\n",
    "\n",
    "print('total pos docs : '+str(len(poss)))\n",
    "print('total prob. of first '+ str(k)+' word in positive group is : '+ str(positives/len(poss_transition[0])))\n",
    "print(\"size of dictionary is: \"+ str(len(poss_transition[0])))\n",
    "print('---------------------------------')\n",
    "\n",
    "print('total neg dics: '+str(len(negs)))\n",
    "print('total prob. of first '+ str(k)+' word in negative group is : '+ str(negatives/len(negs_transition[0])))\n",
    "print(\"size of dictionary is: \"+ str(len(negs_transition[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
